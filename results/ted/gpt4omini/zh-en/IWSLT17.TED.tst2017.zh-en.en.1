Today's computers are so amazing that we often overlook just how terrible they actually are.
Today, I want to discuss this issue with you and how we can use neuroscience to address it.
First, I want to take you back to a cold night in Harlem in 2011, a night that holds profound significance for me.
I was sitting in a bar next to Columbia University, where I was majoring in computer science and neuroscience, having an interesting conversation with a classmate about the power of holography, which would one day replace computers.
Just as our conversation reached a thrilling point, his phone rang, as expected.
He picked up his phone and started typing with his head down.
Then he occasionally looked up at me and said, "Keep talking, I'm listening."
But it was clear that his gaze was not focused, and a wonderful moment had slipped away in an instant.
At the same time, across from the bar, I noticed another student holding his phone, this time surrounded by a group of people in front of the screen.
He was scrolling through photos on Instagram, where the kids were laughing out loud.
Confronted with the same technology, they felt such joy, while I felt an overwhelming sense of frustration; this stark contrast immediately prompted my reflection.
As I delved deeper into my thoughts, I increasingly realized that the obvious issue here was not the electronic information itself, but rather the location of the displayed information that separated me from my friends while bringing these kids together.
It is clear that they are connected by something, much like how our ancestors evolved their social cognition, such as storytelling around a campfire.
I believe this is precisely what tools are meant to do.
They should extend our bodily functions.
I believe that today, computers are doing the exact opposite.
Whether you are sending an email to your wife, composing a symphony, or simply comforting a friend, you are doing so in almost the same way.
You bend over these squares, fiddling with buttons and menus, and more squares appear.
I feel that this is an incorrect approach; I believe we can start using a more natural machine.
The machines we use should be able to bring our work back to real life.
We should use machines that can employ the principles of neuroscience to extend our senses, rather than restrict them.
I happen to have such a machine here now.
It is called Meta 2.
Let's give it a try.
I can now see the audience, and I can also see my hands.
On the count of three, two, one, we will see a holographic image appear, a very realistic holographic image appearing before me, right in front of the glasses I am currently wearing.
Of course, this can be anything we intend to purchase or learn, and I can easily control its movement with my hands.
And I believe Iron Man would be very proud.
We will come back later.
(Audience applauds) If you are thinking the same as I am, you should already be considering what can be done with this technology. Let's take a look at some examples.
My mother is an architect, so it’s only natural that the first thing I imagine is to draw a three-dimensional building, rather than using these flat floor plans.
In fact, she is currently touching the design plans, trying to choose an interior decoration.
These have all been captured by the GoPro in my glasses.
The next use case is quite personal to me, which is Professor Adam Gazzaley's Glass Brain project, and I would like to thank UCSF for the authorization.
As a neuroscience student, I am always amazed by the ability to learn and remember these complex brain structures using a real machine, allowing me to touch and manipulate different brain structures.
What you are seeing now is called augmented reality, but for me, this is just part of a larger story—one that is about how we began to use digital devices to extend our bodies, rather than to restrict our bodily functions.
So—
In the coming years, humanity will undergo a transformation, I believe.
We will begin to overlay an entire layer of digital information onto the real world.
Take a moment to imagine what this means for storytellers, for painters, for neurosurgeons, and for interior decorators, and perhaps for all of us gathered here today.
I believe what we, as a collective, need to do is to truly attempt and strive to imagine how we can create this new reality in a way that extends human experience, rather than gamifying our reality or blending it with digital information.
That is what I am very interested in doing.
Now, I want to share a little secret with you all.
In about five years—this will not be the smallest device—these will look like strip glasses placed in front of you, capable of projecting holographic images.
Just as we don’t pay much attention to the hardware specifications of different phones—what we consider when buying a phone is the operating system— as a neuroscientist, I always hope to build a brain-based iOS, if possible.
It is extremely important that we ensure it functions properly, because the time we spend with these devices may not be any shorter than the time we have spent interacting with the Windows graphical user interface.
I don't know what you all think, but living inside the Windows system feels a bit frightening to me.
To isolate the most intuitive touch interface to infinity, we use neuroscience to guide our design, rather than allowing a group of designers to argue endlessly in the design studio.
The principle of our evolution is called the "minimum resistance neural pathway."
At every moment of change, we connect the brain-based iOS system with our brains, for the first time, using the language of our brains.
In other words, we are attempting to create a computer with a zero learning curve.
We are building a system that you have always known how to use.
These are the first three design guidelines we used in this new form of user experience.
First, you are the operating system.
Traditional file systems are complex and abstract, requiring your brains to navigate multiple twists and turns to decode them.
We are taking the opposite route to the "minimum resistance neural pathway."
At the same time, in augmented reality, you can certainly place your holographic TED display here, put your holographic emails on the other side of the table, and your spatial memory is evolving just right to accurately retrieve this information.
You can place the holographic image of the Tesla you are currently shopping for here—or any model that my legal team tells me about before going on stage.
(Laughter from the audience) Very well, your brains do know how to return to reality.
The second interface guide is referred to by us as "Touch Vision."
What do babies do when they see something that interests them?
They will try to reach out and grab it.
Natural machines work in the same way.
In fact, the visual system receives a fundamental drive that we can refer to as proprioception—this is the sense of where our body parts are in space.
By directly engaging with our work objects, we not only gain better control over them but also develop a deeper understanding of them.
This is "Touch Vision."
However, it is not enough for us to experience things ourselves.
We are social primates.
This brings to mind our third guideline, derived from the "Holographic Campfire" of our first story:
Our mirror—the neural subsystem—shows that we can better connect with each person and our work if we can see a three-dimensional image of everyone's face and hands.
So, if you look at the video behind me, you can see two Meta users interacting with the same holographic image, establishing eye contact and focusing on it, rather than being distracted by external devices.
Let’s try again with the concepts of neuroscience.
Let’s try again, our favorite interface, the brain's iOS system.
I will now take a step forward, walk over to grab these glasses, and then place them here on the table.
I am now in a moment with all of you, and we are connecting together.
My spatial memory is functioning; I can go over, grab it, and then bring it back here to remind myself that I am an operating system.
Now my proprioception is at work; I can go over and shatter these glasses into thousands of pieces, and then touch the sensor that is currently scanning my hand.
However, it is not enough to just see these things, so soon, my co-founder Ryan will give me a 3D call—Ryan?
Hi, Ryan, how are you?
Everyone, I can see this person appearing in front of me in a three-dimensional projection.
Moreover, his image is very lifelike.
(Audience applauds) Thank you.
My mirror—the neural subsystem—shows that this will replace phones in the near future.
Hi, Ryan, how are you?
Ryan: I'm doing well. We can finally do a live demonstration.
(Audience applauds) Ryan, give everyone a holographic brain as a little gift, like we saw in the short film earlier.
Ladies and gentlemen, this will not only change the way we make phone calls, but it will also transform the way we collaborate.
Thank you very much.
Thank you, Ryan.
Ryan: You're welcome.
(Audience applauds) Mike Long Greibitz: So everyone, this is the insight I discovered in that bar in 2011: the future of computers is not locked inside these screens.
But rather here, within our bodies.
(Audience applauds) So if I can leave you with one thought today, it is that natural machines do not only exist in the imagination of the future; they are here, present in 2016.
This is why, before TED 2017, hundreds of employees at Meta—including administrative staff, executives, designers, and engineers—will discard our external display devices and replace them with a truly more natural machine.
Thank you very much.
(Audience applauds) Thank you.
Thank you, everyone.
Chris Anderson: I have a question; there were many demonstrations of holography last year.
Sometimes there is a debate in the field of technology about whether we are truly seeing real things on the screen.
This is a problem that exists in this field; to some extent, technology is presenting a broader perspective than what you actually see while wearing glasses.
Did we just see a real effect?
Mike Long Greibitz: It is absolutely a real effect.
Furthermore, we also employed additional measurement techniques, using GoPro to capture through real lenses, resulting in the various short clips you see here.
We want to try to realize the experience of this world through what we see with the glasses, rather than just seeing parts of it.
CA: Thank you very much for your presentation.
MG: Thank you very much.

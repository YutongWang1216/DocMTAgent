Modern computers are so advanced that we don't even notice how poor their performance can be.
Today, I would like to talk about this issue and how we can "repair" it in neuroscience.
On a night in Harlem in 2011, when the frost fell, that night left a profound impact on me.
I was at a bar outside Columbia University, where I was studying computer science and neuroscience, having a meaningful conversation with a fellow student about the power of holograms that would one day replace computers.
At the peak of our conversation, yes, his smartphone lit up.
He took out his smartphone, looked at it, and began to type.
And looking up at me with just his eyes, he said, "Go on, I'm listening."
But of course, his gaze became hazy, and the meaning of that moment was lost.
At that moment, across the bar, another student held up their smartphone to a group of friends.
They were laughing uncontrollably while showing each other photos on Instagram, swiping through them.
I began to think about how, despite feeling dissatisfied with the same technology, they seemed to be happy.
As I reflected on this, I realized that the "villain" was not the digital information itself, but rather that the display device was what separated me from my friends, while on the other hand, it connected them.
They were connected around something, much like our ancestors who developed social recognition by sharing stories around a campfire. I believe this tool exists for that purpose.
It should be an extension of bodily functions.
In today's computers, the opposite is happening.
When you are sending an email to your wife, composing a symphony, or comforting a friend, you are engaging in remarkably similar actions.
While crouching, I am manipulating the endlessly appearing icons and menu buttons displayed in this square format.
I think this is wrong. I believe we should be able to start using much more "natural machines."
We should use machines that bring these tasks back into the real world.
We should use machines that apply the principles of neuroscience to enhance perception rather than limit it.
Well, here I have brought such a machine.
This is "Meta 2."
Let's give it a try.
Now, I can see all of you in front of me, and I can also see my own hands.
Alright, in 3, 2, 1, we will enter the immersive hologram experience. A very realistic hologram is now appearing right in front of me—right in front of the headset I am wearing.
Of course, what you are looking at could be shopping items or educational materials, and you can use your hands to precisely control and maneuver them effectively.
Iron Man would be proud as well.
I will show you this again shortly.
(Applause) If you think like me, you are probably excited about the possibilities of what can be achieved with such technology. Now, let's take a look at a few examples.
My mother is an architect, so what naturally came to mind was to create building designs in three-dimensional space instead of two-dimensional blueprints.
My mother is currently selecting the interior design by interacting with the visuals.
This was all filmed using a GoPro camera through the headgear.
The next example of usage is something very personal: the "Glass Brain" project conducted by Professor Adam Gazzaley, with the kind support of the University of California, San Francisco.
As a student of neuroscience, I have always dreamed of using machines that allow me to interact with and manipulate the diverse and complex structures of the brain while learning and acquiring knowledge.
What you are seeing now is augmented reality. However, this is part of a more important concept—one that aims to enhance physical abilities rather than limit them through digital devices.
Alright...
I believe that humanity will reach a turning point in the coming years.
All digital information is beginning to be expressed in the real world.
Just imagine for a moment what this means for storytellers, painters, neurosurgeons, interior designers, and—us who are here.
What we should do as a social community is to imagine how we can create a new reality that enhances human experiences, rather than merely gamifying reality or cluttering it with digital information.
I am very passionate about this.
Now, let me reveal a little secret.
In about five years, this is not a small device, but within approximately five years, all of these will become slim glasses that project holograms.
When we choose smartphones, we often overlook the hardware itself and focus on the operating system. However, as a neuroscientist, I have always dreamed of creating what you might call the "iOS of the mind."
It is very important to create this properly, as we may spend a period of time with this kind of system that is at least as long as we have spent with the Windows interface.
I don't know about you all, but living inside Windows is a frightening thing.
We have made neuroscience the foundation of our design to derive the most intuitive interface from all possibilities, instead of having designers clash in the boardroom.
And the principle at the core of this is what is known as the "minimum resistance neural pathway."
Continuously connecting the "iOS of the mind" to the human brain—this is the first time that the human brain will control that iOS.
In other words, we are trying to create computers that do not require learning how to use them.
We are creating a system that allows users to intuitively understand how to use it.
I will introduce the three main new design guidelines for user experience that we have employed.
The most important thing is that "the user is the operating system."
The traditional file-based systems are complex and abstract, requiring us to engage our minds just to understand them.
It goes against the "minimum resistance neural pathway."
However, in augmented reality, by placing a holographic TED panel here and a holographic email on the other side of the desk, spatial memory can adapt to this, allowing you to reach out and retrieve information.
We can display anything in hologram form, including what the legal team requested from me prior to the presentation about the Tesla car I want to buy.
Alright, now your brain has learned how to retrieve information.
The second guideline for the interface is "See and Touch."
What does a baby do first when it sees something it is interested in?
It reaches out and tries to touch it, doesn't it?
A "natural machine" should work in this way.
The visual system receives basic stimuli from what is known as proprioceptive sensation, which occurs deep within the body.
Therefore, by directly touching the work, not only can you control it better, but you can also understand it more deeply.
Therefore, it is "See and Touch."
However, this alone is not sufficient as an experience.
Humans are inherently social primates.
From there, the third guideline, that is, the holographic version of the campfire example introduced at the beginning, is derived.
The primate mirror neuron subsystem indicates that if individuals can perceive each other's faces and hands in three dimensions, they can collaborate and work together more effectively.
Please look at the video behind me, where two Meta users are interacting with the same hologram, maintaining eye contact without being distracted by their devices, connecting through this object.
Now, let's try this again with neuroscience in mind.
Our favorite, the "iOS of the mind."
Let's take it a step further. I'll pick up this headset and place it here beside the desk.
Right now, I am here with all of you, and in this moment, we are connected.
As my spatial memory kicks in and I grab the headset to retrieve it, I realize that I am the operating system.
And as my proprioceptive sensation kicks in, I deconstruct this headset into thousands of parts—touching the sensors that scan my hands.
But just watching this alone is not enough, so now my co-founder Ray will be making a 3D call shortly—Ray?
(Ringing) Hey Ray, how are you?
Everyone, I can see him in three dimensions right in front of me.
with a photographic sense of reality
(Applause) Thank you very much.
My mirror neuron system suggests that this will soon replace the mobile phone.
Hey Ray, how's it going?
Sure, we’re having a live conversation now.
(Applause) Ray, let me show you all the holographic brain we saw in the video earlier.
Everyone, this will not only change the phone but also transform the way we collaborate.
Thank you very much.
Ray, thank you.
You're welcome.
(Applause) This is the message I discovered in that bar in 2011: the future of computers is not confined to flat screens.
Here, it exists within us.
(Applause) If I were to leave you with just one idea today, it would be that "natural machines" are not something of the future; they are already here in 2016.
Therefore, all 100 staff members at Meta—including administrative personnel, executives, designers, and engineers—will have discarded monitor screens and will be using truly "natural machines" by TED 2017.
Thank you very much.
(Applause) Thank you very much.
Thank you.
(Chris Anderson) So I would like to ask—over the past few years, there have been several demonstrations of augmented reality, but...
"There is also occasional debate among engineers about whether we are actually seeing things on a large screen."
There were also comments regarding the field of view, noting that during the demonstration, a wider field of view could be portrayed than what is actually visible when wearing the headset.
"Is what you showed us the actual footage?"
"Of course, it's the real thing."
Not only that, but when creating the video I introduced, I particularly made an effort to shoot through the lens of a GoPro camera.
"I wanted you all to fully experience the genuine experience as seen through the headgear in a simulated way."
Thank you for the introduction.
Thank you very much.

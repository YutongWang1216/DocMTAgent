Modern computers are so amazing that we even overlook how bad they actually are.
Today I want to discuss this issue with you and how we can solve it using neuroscience.
First, I want to take you back to a cold night in Harlem in 2011, a night that holds deep significance for me.
I was sitting in a bar next to Columbia University, where I majored in computer science and neuroscience. I was having a very interesting conversation with a classmate about the power of holographic photography, which will one day replace computers.
Just as our conversation reached its climax, as expected, his phone rang.
He picked up his phone and started typing with his head down.
Then he would occasionally look up at me and say, "Go on, I'm listening."
But it was obvious that his gaze was not focused, and a wonderful moment passed by in an instant.
At the same time, across the bar counter, I noticed another student holding his phone, with a group of people gathered around the screen this time.
He was flipping through photos on Instagram, while the children were laughing out loud.
Facing the same technology, they felt so happy, while I felt extremely frustrated, this strong contrast immediately sparked my contemplation.
As I delved deeper into my thoughts, I increasingly realized that the issue here is not electronic information, but simply the location where the information is being displayed that separates me from my friends, yet brings these children together.
It is obvious that they are being connected by something, just like our ancestors evolved their social cognition, such as telling stories around the campfire.
I believe this is exactly what tools should do.
They should extend our bodily functions.
I believe that in today's world, computers are doing exactly the opposite.
Whether you are sending an email to your wife, composing a symphony, or just comforting a friend, you are using almost the same method.
You bend over the blocks, manipulate buttons and menus, and more blocks appear.
I think this is a wrong way, I think we can start using a more natural machine.
The machines we use should be able to bring our work back to real life.
We should use machines that can utilize principles of neuroscience to extend our senses, rather than limiting them.
Now I happen to have a machine like that here.
It's called Meta 2.
Let's give it a try.
Now I can see the audience, and I can see my hands.
When we count down to 3, 2, 1, we will see a holographic image appear, a very realistic holographic image appearing in front of me, in front of the glasses I am wearing now.
Of course, this could be anything we are planning to purchase or learn, and I can easily control its movement with my hands.
And I believe Iron Man would be proud.
We will come back later.
(Applause from the audience) If you are thinking the same as me, you should already be thinking about what can be done with this technology. Let's take a look at some examples.
My mother is an architect, so naturally the first thing I imagine is to draw a three-dimensional building, rather than using these flat floor plans.
In fact, she is now touching the design drawings, trying to choose an interior decoration.
All of these were captured by the GoPro in my glasses.
The next use case is very personal to me, it is Professor Adam Gazzaley's Glass Brain project, for which I thank UCSF for the authorization.
As a neuroscience major student, I am always amazed by the ability to learn and memorize these complex brain structures using a real machine, so that I can touch and manipulate different brain structures.
What you are seeing now is called augmented reality, but for me, it is just a part of a more important story - a story about how we are starting to use digital devices to extend our bodies, rather than limit our physical functions.
Soâ€”
In the coming years, humanity will undergo a transformation, I believe.
We will begin to overlay a whole layer of digital information in the real world.
Take a moment to imagine what this means for storytellers, for painters, for neurosurgeons, for interior decorators, and perhaps for all of us here today.
I believe what we as a group need to do is truly try and strive to imagine how we can create this new reality in a way that extends the human experience, rather than gamifying our reality or blending reality with digital information.
That is something I am very interested in doing.
Now, I want to tell you a little secret.
Within about five years - this will not be the smallest device - within about five years, these will all look like strip glasses placed in front of the eyes, capable of projecting holographic images.
Just as we don't really care about the hardware parameters of different phones - when we buy a phone, we consider the operating system - as a neuroscientist, I always hope to build a brain-based iOS, if possible.
It is very, very important that we ensure it operates smoothly, because the time these things accompany us may not be shorter than the time we have interacted with the Windows graphical user interface.
I don't know about you, but living inside the Windows system kind of scares me a bit.
In order to isolate the most intuitive interface contact to an infinite distance, we use neuroscience to guide our design, rather than letting a group of designers argue endlessly in the design room.
The principle of our evolution is called the "minimal resistance neural pipeline".
At every moment of change, we connect the brain-based iOS system with our brains, for the first time, using the language of our brains.
In other words, we are trying to create a computer with zero learning curve.
We are building a system that you have always known how to use.
These are the earliest three design guidelines we used in this new form of user experience.
First, you are the operating system.
Traditional file systems are complex and abstract, requiring your brain to take several extra steps to decode them.
We are taking a route opposite to the "minimal resistance neural pipeline."
At the same time, in augmented reality, you can certainly place your holographic TED display screen here, put your holographic email on the other side of the table, and your spatial memory has evolved to accurately retrieve this information.
You can place the holographic image of the Tesla you are shopping for here - or any patterns my legal team informs me of before going on stage.
(Audience laughter) Very good, your brain indeed knows how to come back to reality.
The second interface guideline is referred to as "touchable visual."
How do babies react when they see something that interests them?
They will try to reach out and grab it.
Natural machines also work in this way.
In fact, the visual system receives a very basic driving force, which we can call proprioception - a sense of where our body parts are in space.
By directly interacting with our work object, we not only control it better, but also gain a deeper understanding of it.
This is "touchable visual."
But experiencing things ourselves is not enough.
We are social primates.
This reminds me of our third guideline, from our first story "Holographic Campfire":
Our mirror - the neural subsystem shows that we can better connect with everyone and our work if we can see three-dimensional images of each person's face and hands.
So, if you look at the video behind me, you can see two Meta users operating the same holographic image, establishing eye contact, focusing on this thing, rather than being distracted by external devices.
Let's try again with the concept of neuroscience.
Let's try again, our favorite interface, the brain's iOS system.
I will now take a step forward, walk to the front and pick up this pair of glasses. Then I will place it on the table here.
I am now in the same moment as you, we are connected.
My spatial memory is working, I can reach back and bring it here, reminding myself that I am an operating system.
Now my proprioception is working, I can reach back and shatter these glasses into thousands of pieces, then touch the sensors scanning my hand at this moment.
But just seeing these is not enough, so soon, my co-founder Ryan will make a 3D call to me - Ryan?
Hi, Ryan, how are you?
Everyone, I can see this person appearing in front of me in the form of a three-dimensional projection.
And his image is very lifelike.
(The audience applauds) Thank you.
My mirror - the neural subsystem shows that this will replace phones in the near future.
Ryan, how are you?
Ryan: I'm good. We can finally do a live demonstration now.
(Applause from the audience) Ryan, give everyone a little gift of the holographic brain we saw in the short film earlier.
Everyone, this will not only change the way we make phone calls, but also the way we collaborate.
Thank you very much.
Thank you, Ryan.
Ryan: You're welcome.
(The audience applauds) Melon Gribez: So, everyone, this is the information I discovered in that bar in 2011: the future of computers is not locked inside these screens.
But here, inside our bodies.
(The audience applauds) So if I can leave you with one idea today, it's that natural machines are not just a figment of the future, they are right here, existing in 2016.
This is why at Meta, hundreds of employees, including executives, executives, designers, engineers - before TED2017, we will throw away our external display devices and replace them with a truly more natural machine.
Thank you very much.
(The audience applauds) Thank you.
Thank you, everyone.
Chris Anderson: I have a question, there have been many exhibitions about holographic photography last year.
Sometimes there is a debate in the field of technology about whether we are really seeing real things on the screen or not.
This is a problem that exists in this field, in a sense, technology is presenting a broader perspective than what you actually see through glasses.
Did we just see a real effect?
Melon Gribez: It is definitely a real effect.
Not only that, we also used additional measurement techniques and filmed through real lenses with GoPro, resulting in the various short films you see here.
We want to try to make the experience of the world more real through what we see through glasses, rather than just seeing parts of it.
CA: Thank you very much for your presentation.
MG: Thank you very much.

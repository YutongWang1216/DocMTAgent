Today's computers are so amazing that we don't realize how bad they are.
I would like to talk to you today about this issue and how we can solve it with neuroscience.
First, I would like to take you back to Harlem on a freezing night in 2011 that deeply marked me.
I was sitting at a bar not far from Columbia University, where I was studying computer science and neuroscience, and I was in deep conversation with another student about the potential of holograms to one day replace computers.
As we reached the best part of the conversation, of course, his phone lights up.
He takes it out, looks at it, and starts typing something.
Then he forces his eyes back to me and says, "Go on. I'm listening."
But of course, his eyes were absent, and the moment had passed.
Meanwhile, in this bar, I saw another student holding his phone, this time towards a group.
He was scrolling through photos on Instagram, and those kids were laughing hysterically.
This dichotomy between my feeling of discomfort and the joy they felt about the same technology made me question.
The more I thought about it, the more I realized that here, the problem was clearly not digital information, it was only the position of use that separated me from my friend and brought these kids together.
They were connected around something just like our ancestors who improved their social knowledge by telling stories around the campfire.
I think this is exactly what tools should do.
Extending our bodies.
I think that today computers do the opposite.
Whether you send an email to your wife, compose a symphony, or console a friend, you do it in pretty much the same way.
You are leaning over these rectangles, tapping on buttons, menus, and more rectangles.
I think that's the wrong way. I think we can start using a more natural machine.
We should use machines that bring our work back into this world.
Machines that use the principles of neuroscience to extend our senses, rather than going against them.
It turns out that I actually have such a machine.
Its name is Meta 2.
Let's test it!
In front of me, I can see an audience, and I can see my hands.
And in three, two, one, we will see an immersive hologram, a very realistic hologram appears in front of me, coming from the glasses I am wearing.
Of course, this could be something we buy or learn, and I can use my hands to move them delicately.
I think Iron Man would be proud.
We will come back to this a little later.
Now, if you are like me, your mind is already reeling from the possibilities we have with this type of technology, let's look at a few of them.
My mother is an architect, so naturally the first thing I imagined was to draw a building in 3D instead of using those 2D plans.
There, she touches graphs and selects an interior decor.
Everything was filmed with a GoPro using our own glasses.
This next case is really special to me, it is about Professor Adam Gazzaley's brain glasses project, provided by UCSF.
As a neuroscience student, I will always fantasize about the possibility of learning and memorizing these complex brain structures with a real machine, where I could touch and play with these different brain structures.
What you see now is called augmented reality, but for me, it is part of a much larger story - a story about how we are beginning to extend our bodies through digital objects, rather than the other way around.
Now...
I think that in the coming years, humanity will experience a change.
We will begin to overlay an entire layer of digital information onto the real world.
Imagine for a moment what this would mean for storytellers, painters, neurosurgeons, interior decorators, and perhaps for all of us here today.
And what we must do as a community is to try and make the effort to imagine how we can create this new reality in a way that extends the human experience, rather than gamifying our reality or cluttering it with digital information.
That is what truly excites me.
Now, I want to tell you a little secret.
In about five years - it's not the smallest device - in about five years, they will all look like glass bands on our eyes projecting holograms.
Just as we care little about the phone we buy for its technical specifications - we buy it for the operating system - as a neuroscientist, I have always dreamed of creating the "iOS of the mind," if you will.
It is very, very important to do things right, as we could live in these things for at least as long as the time spent with the Windows user interface.
And I don't know about you, but living inside Windows scares me.
To isolate the most intuitive interface from the infinite, we use neuroscience to create our design principles, rather than a group of designers arguing in a meeting.
The principle we all rely on is what is called the "Neural Path of Least Resistance."
Each time, we try to connect the iOS of our brain with our brain according to, for the very first time, our conditions.
In other words, we are trying to create a computer without a learning curve.
We are building a system that you have known how to use forever.
Here is the first of the three principles we use in this brand new user experience.
First and foremost, you are the operating system.
Traditional file systems are complex and abstract, requiring extra steps from your brain to decode them.
We are going against the Neural Path of Least Resistance.
Meanwhile, in augmented reality, you place your TED holographic panel right there, and your holographic email on the other side of the desk, and your evolved spatial memory retrieves them.
You could place your holographic Tesla that you purchase - or any model that my legal team told me to put.
Perfect. Your brain knows exactly how to bring it back.
We call the second interface principle "Touch to see".
What do babies do when they see something that catches their attention?
They try to reach it, to touch it.
This is exactly how the natural machine should work.
It turns out that the visual system receives a crucial boost from a sense called proprioception - it is the sense of our body's limbs in space.
By directly touching our work, we will not only have better control over it, but we will also have a better understanding of it.
And thus, touch to see.
But it is not enough to feel things.
We are inherently social primates.
This brings me to our third principle, the holographic campfire from our first story.
Our mirror neuron system suggests that we can better connect with each other and with our work if we can see our faces and hands in 3D.
So if you look at the video behind me, you can see two Meta users playing with the same hologram, establishing eye contact, connected around this object, instead of being distracted by external devices.
Let's continue and try again, keeping neuroscience in mind.
Once again, our favorite interface, the iOS of the mind.
I will now go further and grab this pair of glasses and leave it here next to the desk.
I am with you, right now, we are connected.
My spatial memory has kicked in, and I can grab it and put it back here, remembering that I am the operating system.
And my proprioception is active, and I can continue and shatter these glasses into thousands of pieces and touch the sensor that is scanning my hand.
But it's not enough to see objects alone, in a second, co-founder Ray will make a 3D phone call - Ray?
Hi Ray, how are you?
I can see this person fully in front of me in 3D.
And he is quite realistic.
Thank you.
My mirror neuron system suggests that this will replace phones in no time.
Ray, how are you?
Ray: Very well. We are live today.
MG: Ray, give the audience a gift of the holographic brain that we saw earlier.
This will not only replace phones, but it will also change our way of collaborating.
Thank you very much.
Thank you, Ray.
Ray: You're welcome.
MG: Well, here is the message I discovered in a bar in 2011: the future of computers is not confined inside one of these screens.
It is right there, within us.
If there is only one idea I can give today, it is that the natural machine is not the product of the future, it is right here in 2016.
That's why all of us at Meta, including administrative staff, executives, designers, engineers -- before TED2017, we will all throw away our external monitors and replace them with machines that are truly and deeply more natural.
Thank you very much.
Thank you, that's very kind.
Thank you to all.
Chris Anderson: Well, explain to me one thing, because there have been few demonstrations on augmented reality last year or previously right here.
And there is sometimes a debate among technologists about whether we really see the real object on the screen.
It's a field of vision issue. In a way, the technology shows a wider view than what you should see when wearing the glasses.
Have we seen real life?
MG: Absolutely the real life.
And not only that, we have taken other steps to film with a GoPro using the real lenses, the various videos broadcasted.
We want to try to simulate the experience of the world that we see through the glasses, and not cut any passages.
CA: Thank you very much for this presentation.
MG: Thank you very much, it was a pleasure.

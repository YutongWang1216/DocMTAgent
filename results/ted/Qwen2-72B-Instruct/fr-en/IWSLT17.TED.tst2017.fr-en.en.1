Today's computers are so incredible that we don't realize how bad they actually are.
I would like to talk to you today about this problem and how we can solve it with neuroscience.
Firstly, I would like to take you back to Harlem on a freezing night in 2011 that left a deep impression on me.
I was sitting at a bar not far from Columbia University, where I was studying computer science and neuroscience, and I was engaged in a deep conversation with another student about the potential for holograms to one day replace computers.
As we were reaching the most interesting part of the conversation, of course, his phone lit up.
He takes it out, looks at it, and starts typing something.
Then he forces his eyes back to me and says, "Go on. I'm listening."
But of course, his eyes were elsewhere, and the moment had passed.
Meanwhile, in this bar, I saw another student holding his phone, this time towards a group.
He was scrolling through pictures on Instagram, and these kids were laughing hysterically.
This dichotomy between my feeling of discomfort and the joy they felt about the same technology made me wonder.
The more I thought about it, the more I realized that here, the issue was clearly not the digital information, it was merely the usage position that separated me from my friend and brought these kids together.
They were connected around something just like our ancestors who enhanced their social knowledge by telling stories around the campfire.
I think that's exactly what tools should do.
Extending our bodies.
I think that today's computers do the opposite.
Whether you're sending an email to your wife, composing a symphony, or consoling a friend, you do it pretty much the same way.
You are hunched over these rectangles, tapping on buttons, menus, and more rectangles.
I think that's the wrong approach. I think we can start to use a machine in a more natural way.
We should use machines that bring our work back into this world.
Machines that use the principles of neuroscience to extend our senses, rather than working against them.
It turns out that I actually have such a machine.
It is called Meta 2.
Let's test it!
In front of me, I can see an audience, and I can see my hands.
And in three, two, one, we will see an immersive hologram appear, a very realistic hologram appears in front of me, coming from the glasses that I wear.
Of course, this could be something we buy or learn, and I can use my hands to manipulate them finely.
I think Iron Man would be proud.
We will come back to this a little later.
Now if you're like me, your mind is already reeling from the possibilities we have with this kind of technology, let's look at a few of them.
My mother is an architect, naturally the first thing I envisioned was drawing a building in 3D instead of using these 2D plans.
There, she touches the charts and selects an interior decor.
Everything was filmed with a GoPro using our own glasses.
This next case is particularly special to me, it's about Professor Adam Gazzaley's brain glasses project, provided by UCSF.
As a neuroscience student, I will always daydream about the possibility of learning and memorizing these complex brain structures with a real machine, one I could touch and play with these various brain structures.
What you see now is called augmented reality, but to me, it's part of a much bigger story - a story about how we begin to extend our bodies through digital objects, rather than the reverse.
Now...
I believe that in the coming years, humanity is going to experience a change.
We are going to start laying an entire layer of digital information over the real world.
Imagine for a moment what this would mean for storytellers, painters, neurosurgeons, interior decorators, and perhaps all of us here today.
And what we need to do as a community is try and make the effort to imagine how we can create this new reality in a way that extends the human experience, rather than gamifying our reality or cluttering it with digital information.
This is what truly fascinates me.
Now, I want to tell you a little secret.
In about five years - it's not the smallest device - in about five years, they will all look like strands of glass over our eyes that project holograms.
Just as we care little about the technical specifications of the phone we buy - we buy it for the operating system - as a neuroscientist, I have always dreamed of creating the "iOS of the mind", so to speak.
It is very, very important to do things right, because we could be living with these things for at least as long as we have spent with the Windows user interface.
And I don't know about you, but living inside Windows scares me.
To isolate from infinity the most intuitive interface, we use neuroscience to create our design principles, rather than a bunch of designers squabbling in a meeting.
The principle on which we all rely is what is called the "Path of Least Resistance Neural Pathway."
Every time, we attempt to connect our brain's iOS with our brain itself, for the very first time, on our own terms.
In other words, we are trying to create a computer without a learning curve.
We are building a system that you have always known how to use.
Here is the first of the three principles that we apply in this entirely new user experience.
Firstly, you are the operating system.
Classic file systems are complex and abstract, requiring extra steps from your brain to decode them.
We counteract the Path of Least Resistance Neural Pathway.
Meanwhile, in augmented reality, you place your TED holographic panel right there, and your holographic email on the other side of the desk, and your advanced spatial memory retrieves them.
You could place your holographic Tesla that you buy - or any model that my legal team has instructed me to include.
Perfect. Your brain knows exactly how to bring it back.
We call the second interface principle "Touch to See".
What do babies do when they see something that catches their attention?
They try to reach for it, to touch it.
This is exactly how the natural machine should work.
It turns out that the visual system gets an essential boost from a sense we call proprioception â€” it's the sense of the position of our body parts in space.
By directly touching our work, we will not only control it better, but we will also understand it better.
And thus, touch to see.
But it's not enough to feel things.
We are inherently social primates.
This brings me to our third principle, the holographic campfire of our first story.
Our mirror neuron system suggests that we can better connect with each other and with our work if we can see our faces and hands in 3D.
So if you look at the video behind me, you can see two Meta users interacting with the same hologram, making eye contact, connected around this object, instead of being distracted by external devices.
Let's move forward and try again with neuroscience in mind.
Once again, our favorite interface, the iOS of the mind.
I am now going to go further and grab this pair of glasses and leave it here next to the desk.
I am with you, right now, we are connected.
My spatial memory has kicked in, and I can grab it and put it back here, remembering that I am the operating system.
And my proprioception is active, and I can continue and break these glasses into thousands of pieces and touch the sensor that is scanning my hand.
But seeing objects alone is not enough, in a moment, co-founder Ray is going to call in 3D -- Ray?
Hello Ray, how are you?
I can see this person fully in front of me in 3D.
And it is quite realistic.
Thank you.
My mirror neuron system suggests that this will replace phones in no time.
Ray, how are you?
Ray: Very well. We are live today.
MG: Ray, give the audience a gift from the holographic brain we saw earlier.
This will not only replace phones, but it will also change the way we collaborate.
Thank you very much.
Thank you, Ray.
Ray: You're welcome.
MG: Alright, here's the message I uncovered in a bar in 2011: the future of computers is not confined within any of these screens.
It is right there, within us.
If there's only one idea I can impart today, it's that the natural machine is not a product of the future, it is right here in 2016.
That's why all of us at Meta, including the administrative staff, executives, designers, engineers -- before TED2017, we're all going to throw out our external monitors and replace them with machines that are truly and deeply more natural.
Thank you very much.
Thank you, that's very kind.
Thank you all.
Chris Anderson: Alright, explain one thing to me, because there have been few demonstrations on augmented reality here last year or previously.
And sometimes there is a debate among technologists about whether we actually see the real object on the screen.
It's a field of view issue. In a way, the technology shows a wider view than what you should see wearing the glasses.
Have we seen real life?
MG: Absolutely real life.
And not only that, we've taken other steps to film with a real GoPro lens, the various videos being broadcasted.
We want to try to simulate the experience of the world as we see it through the glasses, and not cut any part of it.
CA: Thank you very much for this presentation.
MG: Thank you very much, it was a pleasure.

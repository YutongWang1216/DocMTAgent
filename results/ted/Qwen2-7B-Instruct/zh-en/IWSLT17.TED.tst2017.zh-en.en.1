The wonders of modern computers have made us overlook just how bad they they really are.
Today, I want to discuss this issue with you and how we can address it using neuroscience.
First, I want to take you back to a cold night in Harlem in 2011, a night that had a profound significance for me.
I was sitting in a bar next to Columbia University, where I majored in computer science and neuroscience, having a fascinating conversation with a classmate about the power of holography, which would one day replace computers.
Just as our conversation reached its peak of interest, unsurprisingly, his phone rang.
He picked up the phone and bent down to start typing.
Then he would occasionally look up at me and say, "Go on, I'm listening."
However, it was was clear that his gaze was not focused, such a wonderful moment passed by in an instant.
Simultaneously, across from the bar, I noticed another student holding his phone, this time with a group of people gathered around the screen.
He was scrolling through Instagram photos, where kids were laughing loudly.
Facing the same technology, they found it so delightful, while I felt immensely depressed by it. This stark contrast immediately sparked my contemplation.
As I delved deeper into my thoughts, I increasingly realized that the clear issue here wasn't not the electronic information, but simply the location where information was presented, which had separated me and my friends, yet brought these kids together.
It was abundantly clear that they are bound together by something, much like how our ancestors evolved their social cognition, for example, around the campfire telling stories.
I believe, this is precisely what tools should be doing.
They should extend our bodily capabilities.
I believe that, today, computers are doing exactly the opposite.
No matter whether you're sending an email to your wife, composing music for an orchestra, or simply comforting a friend, you're almost always using the same manner.
You bend over these blocks, playing with buttons and menus, more blocks appear.
I feel this is a wrong way, I think we should start using a more natural machine.
The machines we use should be able to bring our work back to the real world.
We should use machines that leverage principles of neuroscience to extend our senses, not constrain them.
Now I have such a machine right here.
It's called Meta 2.
Let's give it a try.
Now I can see the audience, as well as my own hands.
Counting down to three, two, one, we will see a holographic image appear, a very realistic hologram right in front of me, in front of the glasses I am currently wearing.
Of course, this could be anything we're about to buy or learn, and I can easily manipulate its movement with my hands.
And I believe Iron Man would be proud.
We'll come back to that later.
(Aaudience applause) If you're thinking like me, you should already be wondering what you can do with this technology. Let's look at some examples.
My mother is an architect, so naturally, the first thing I envisioned was was to create a 3D model of a building, not these flat floor plans.
In fact, she is now touching the design drawings to choose an interior decoration.
These were all captured by the GoPro embedded in my glasses.
The next use case is very personal to me, involving Professor Adam Gazzaley's Brain Game project, for which I am grateful for the authorization from UCSF.
As a neuroscience student, I am always amazed by the ability to learn and remember these complex brain structures using a real machine, allowing me to touch and manipulate different brain structures.
What you're seeing now is called augmented reality, but to me, it's just a part of a bigger story - a story about how we begin to use digital devices to augment our bodies, not to constrain them.
So...
In the years to come, I believe humans will undergo a transformation.
We will begin overlaying a full layer of digital information onto the real world.
Take a moment to consider what this means for storytellers, for painters, for neurosurgeons, for interior decorators, perhaps for all of us gathered here today.
What I think we as a collective should be doing is truly trying and working hard to imagine how we create this new reality in a way that extends human experience, rather than gamifying our reality or mixing reality with digital information.
That's what I'm very interested in doing.
Now, I want to share a little secret with you.
Within about five years - this won't be the smallest device - within about five years, these will look like strip spectacles placed in front of your eyes, capable of projecting holographic imagery.
Like we don't often obsess over the hardware specs of different phones - it's the operating system that dictates which phone we buy - as a neuroscientist, I've always hoped to build something like iOS for the brain, if possible.
What is absolutely crucial is that we ensure its functionality, because these devices might accompany us longer than our exposure to Windows graphical user interface.
I don't know what you think, but living inside a Windows system makes me feel a bit creepy.
To push the most intuitive single-point of contact infinitely far away, we guide our design with neuroscience, not letting a group of designers argue endlessly in a studio.
Our principle of evolution is called "least resistance neural conduit".
At every moment of change, we connect our brain iOS system to our mind, for the first time using our brain language.
In other words, we're trying to build a computer with no learning curve.
We are building a system that you have always known how to use.
These are the first three design guidelines we used in this new form of user experience.
First, you are the operating system.
Traditional file systems are complex and abstract, making your brain work harder to decipher them.
We are taking the opposite route of the "least resistance neural conduit."
In augmented reality, of course you could put your holographic TED display here, your holographic email on the other side of the desk, and your spatial memory evolves just enough to accurately retrieve that information.
You could put a Tesla hologram here while you're shopping online - or any pattern my legal team told me before going on stage.
(LAudience laughter) Well, your brains do know how to come back down to reality.
The second user interface guideline, which we refer to as "Touch Visual", is now translated to English.
How would infants act when they see something that interests them?
They would try to reach out and grab it.
Natural machines work in the same way.
In fact, the visual system gets a fundamental push, which we can call proprioception â€“ a sense of our body parts being in space.
By directly interacting with our working objects, we not only control them more effectively but also gain a deeper understanding of them.
This is "Touch Vision".
However, simply experiencing things ourselves is not enough.
We are social primates.
This reminds me of our third principle, drawn from our first story, "The Holographic Fire":
Our mirrors - neural subsystems displays - show that we can connect better with everyone and our work if we could see each person's's three-dimensional face and hands.
Therefore, if you look at the video behind me, you'll see two Meta users interacting with the same holographic image, maintaining eye contact, focusing on this thing rather than being distracted by external devices.
Let's try again with the concept of neuroscience.
Again, our favorite interface, the brain's iOS system.
I will move forward now, reach out to grab these glasses, and then place them here on the table.
I am with you in this moment, we are connected together.
My spatial memory is at work, I can go over and grab it, and then bring it back here, reminding myself that I am an operating system.
My proprioception is at work now; I can go over and shatter these glass panels into thousands of pieces, then touch the sensors scanning my hands at this very moment.
But seeing this alone isn't enough, so soon, my co-founder Ryan will be making a 3D call to me - Ryan?
Hi, Ryan, how are you?
Ladies and gentlemen, I can see this person appearing before me in a three-dimensional projection.
And his image is very realistic.
(The audience applauds) Thank you.
My mirror - neural subsystem display - this will replace the telephone soon.
Ryan, how are you?
Ryan, I'm doing well. We can finally demonstrate it live.
(Aaudience clapping) Ryan, give us a holographic brain as a little gift, like the one we saw in the short film before.
Ladies and gentlemen, this will not only change the way we make calls, but it will also transform the way we collaborate.
Thank you very much.
Thank you, Ryan.
Ryan, you're welcome.
(AApplause) Marc Greiber: So, folks, this is the information I found at that bar in 2011, that the future of computers is not locked inside these screens.
And it's right here, within us, in our bodies.
(Applause) So if I can leave you with one idea today, it's that natural machines are not just a product of futuristic imagination; they exist here, in 2016.
This is why at Meta, among hundreds of employees including executives, designers, engineers - before TED 2011, we will ditch our external displays and replace them with a real, more natural machine.
Thank you very much.
(Audience applauds) Thank you.
Thank you everyone.
Chris Anderson: I have a question. There were a lot of holographic demonstrations last year.
Sometimes there's a debate in tech circles about whether what we're seeing on the screen is truly real?
This is a problem within the field, to some extent technology is presenting a broader perspective than what you actually see when wearing glasses.
Were we just witnessing a genuine demonstration?
Marc Greiber: It was absolutely genuine.
In addition, we utilized supplementary measurement techniques, capturing footage through actual lenses using a GoPro, resulting in the various short clips you see here.
We aim to enhance the experience of this world by making it feel real through what we see through the glasses, not just limited to fragmented views.
CA: Thank you very much for your demonstration.
MG: Thank you very much.

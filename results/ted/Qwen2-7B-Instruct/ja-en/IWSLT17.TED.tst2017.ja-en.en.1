Modern computers are so superior that we hardly notice how poor they performances are.
Today, I would like to talk about this issue and - repair it - in the context of neuroscience.
In 2011, during the frosty night in Harlem, that night gave me a profound shock.
It was when I was having been studying computer science and neuroscience at Columbia University, outside a bar, exchanging meaningful conversations with one of my fellow students about the power of holograms that will someday replace computers.
Our conversation reached its climax, then his smartphone lit up.
He took out his smartphone and started typing, directing his gaze towards it.
"And then, looking up at me with just my eyes, he said, 'Keep going, I'm still listening.'"
But of course, his gaze had become vague, and the meaning of that time was was lost.
At that moment, across the bar, another student was holding a smartphone aloft to a group of friends.
As they were swiping through Instagram photos, they were laughing.
I began to think about how they could feel happy despite being unsatisfied with the same technology.
As I thought more deeply, I realized that the "villains" were not the digital information itself, but merely the display devices were separating me from my friends and, on the other hand, they were what bound them together.
They were connected around something, just like our ancestors would gather around a fire to tell stories and develop social cognition. I believe this tool is here for that purpose.
It should be an extension of bodily function.
In today's's computers, the opposite occurs.
When you're sending emails to your wife, composing a symphony, or comforting a friend, you're taking exactly the same actions.
Hunching over, they manipulate these icons and menu buttons which continuously pop up in a square display format.
I believe this is wrong. I think we can start using machines more naturally.
We should use machines that bring such tasks back to the real world.
We should use machines that expand perception rather than limit it, by applying principles from neuroscience.
Here is such a machine that I've brought.
This is "Meta 2".
Let's try it out.
Now before my eyes, everyone can be seen, and even my own hands.
Alright, 3, 2, 1, I'm entering the immersive hologram experience. A very realistic hologram is floating in front of me - right in front of the headset that I'm currently fitting.
Of course, what you're observing could be shopping items, learning materials, or something else. And you can control it precisely with your hands and move it smoothly.
"Iron Man would also be proud of."
This, we will show again shortly.
(Applause) If you think like me, you would be excitedly considering what possibilities such technologies open up. Let's take a look at some examples.
My mother is an architect, so it naturally occurred to me to draw building designs in three-dimensional space instead of on two-dimensional plans.
My mother is currently selecting interior decorations by touching the images.
This was all filmed using GoPro cameras through a headgear.
This next application is is very personal, carried out by Professor Adam Gazzaley at the University of California, San Francisco, with their kind permission, for the "Glass Brain" project.
As a student of neurological science, I've always dreamed of using machines that allow me to interact with and learn from this diverse and complex structure of the brain, much like one would manipulate objects.
What I'm seeing now is augmented reality, but this is part of a more significant idea - to expand rather than limit physical abilities through digital devices.
"Alright..."
I believe humanity will enter a transitional period in the next few years.
All digital information begins to manifest in the real world.
Let's imagine for a moment, this narrative storyteller, painter, neurosurgeon, interior designer, and what does this mean to us, the people here present?
What we should be thinking about as members of society is how to create new realities that expand human experiences, rather than just turning reality into a game or cluttering it up with digital information.
I am very passionate about this.
Let me reveal a little secret.
Within five years, this will not be a small device; within five years, these will all become thin-glassed spectacles that project holograms.
When we choose our smartphones, we tend to prioritize the basic software over the hardware itself. As a neuroscientist, however, I have always dreamed of creating what one might call a 'neural iOS'.
Making this right is is crucial. We might end up spending just as much time with this kind of system as with the window's interface, perhaps for decades to come.
You may not know this, but living inside Windows is is frightening.
We have adopted neuroscience as the foundation for designing the most intuitive interfaces among all possibilities. Instead of having designers debate in executive meetings, we utilize it to derive the simplest and most direct interfaces.
And the principle centered on this is is called 'the least resistance neural pathway'.
Continuously linking "the neural iOS" to the human brain - for the first time, the human brain is controlling its own iOS.
In other words, I'm trying to build a computer that doesn't require learning how to use it.
The system being created allows users to intuitively understand how to use it.
I will introduce the main three design guidelines for the innovative user experience we used.
The most important thing is that 'the user uses the OS'.
Systems composed of traditional files are complex and abstract, requiring you to think even to understand them.
It's going against the 'least resistance neural pathway'.
However, in augmented reality, if you place a hologram TED panel here and put a hologram email on the opposite side of the desk, spatial memory adapts to this, allowing you to reach out and retrieve information.
From the Tesla car I want to buy to everything required by the legal team before my appearance - they can all be displayed holographically.
Alright, now your brain has learned how to retrieve information.
The second guideline for the interface is "see and touch".
What would you do first if a baby shows interest in something?
You're about to reach out and touch, aren't you?
"What a 'natural machine' should work like this.
The visual system receives basic stimuli from something called intrinsic sensory receptors, which are sensations occurring deep within the body.
So, by directly interacting with the work, not only can one control it better, but also understand it more deeply.
"Therefore, it's 'see and touch'."
But this alone isn't sufficient as an experience.
Humans are inherently social primates.
From there, the third guideline is introduced, namely - the holographic version of the bonfire example from the beginning.
The primate mirror neuron subsystem, if capable of seeing each other's faces and hands in three dimensions, demonstrates better coordination and ability to work together.
Please look at the video behind me. Two Meta users are handling the same hologram without being distracted by the device. They are maintaining eye contact and connecting through this object.
Let's try this again with neural science in mind.
Our favorite, the "Neural iOS".
Let's take it one step further. Take this headset and place it beside the desk.
Now I am with you all, in this moment, we are connected.
My spatial memory kicks in, and upon grabbing the headset to retrieve it, I realize that I am the OS.
And my unique sensory perception activates, allowing me to decompose this headset into thousands of parts - I touch the sensor that scans my hand.
But simply watching myself would be insufficient, so shortly, co-founder Ray will be calling us on 3D phone-- Ray?
(Ringing tone) Hey, Ray, how are you?
Everyone, he appears before me in 3D.
With a sense of realism akin to a photograph.
(AApplause) Thank you.
My mirror neuron system suggests that this will soon replace the mobile phone.
Ray, how's you doing?
Okay, we're live chatting right now.
(Aapplause) Ray, let me show you the holographic brain that I demonstrated in the video.
This will not only change how we make calls, but also how we collaborate together.
Thank you.
Ray, thank you.
You're welcome.
(Applause) This is the message I found found at that bar in 2011. The future of computing is not confined to flat screens.
Here, it's within us.
(Applause) If we were to leave just one idea with you all today, it would be that "Natural Machines" are not future endeavors but they already exist in 2016.
Therefore, all 100 staff members at Meta - administrative staff, executives, designers, engineers - will by 2015, will be rid of monitor screens and truly utilizing what are 'natural machines'.
Thank you very much.
(AApplause) Thank you.
Thank you.
(Chris Anderson) I'd like to explore this point-- Virtual reality demonstrations have been conducted several times over the past few years,
The argument also comes up among tech enthusiasts about whether it feels like actually seeing something on a big screen.
There were also comments regarding the field of view. When wearing the actual headset, it's possible to depict a wider field of view than what is visible, which was criticized during the demonstration.
The question is, was the demonstration a real video?
Of course, it's the real thing.
In addition to that, when creating the video I introduced, special efforts were made to shoot through the lens of a GoPro camera.
I wanted everyone to fully experience the genuine, gear-through-the-head, simulated experience just as it is, without any alterations.
Thank you for introducing me.
Thank you very much.

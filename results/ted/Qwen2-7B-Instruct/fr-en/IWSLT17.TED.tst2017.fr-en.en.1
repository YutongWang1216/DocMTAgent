The computers of today are so incredible that we don't realize how bad they are.
I would like to talk to you today about this problem and how we can solve it with neuroscience.
First of all, I would like to take you back to Harlem on a freezing night in 2011 which deeply impacted me.
I was sitting at a bar not far from Columbia University, where I studied computer science and neuroscience, and I was having a great discussion with another student on the ability of holograms to one day replace computers.
As we reach the best moment of the conversation, of course, his phone lights up.
He takes it out, looks at it, and starts to type something.
Then he forces his eyes back to me and says: "Go on. I'm listening."
"But of course, his eyes were absent, and the moment has passed."
Meanwhile, in this bar, I saw another student holding his phone, this time towards a group.
He was scrolling through Instagram photos, and these kids were hysterically laughing.
This contradiction between my feeling of discomfort and the joy they felt about the same technology made me reflect.
The more I thought about it, the more I realized that here, the problem wasn't clearly not digital information, it was was just the usage position that separated me from my friend and gathered these kids.
They were connected around something just like our ancestors who improved their social knowledge by telling stories around the campfire.
I think that's exactly what tools should do.
Extend our bodies.
I think that today's computers are doing the opposite.
Whether you send an email to your wife, compose a symphony, or comfort a friend, you do it about the same way.
You are leaning over these rectangles, pressing buttons, menus and more rectangles.
I think it's the wrong way. I think we can start using a more natural machine.
We should use machines that bring our work back into this world.
Machines that use the principles of neuroscience to extend our senses, rather than going against them.
It turns out that I have such a machine.
It's called Meta 2.
Let's test it!
In front of me, I can see an audience, and I can see my hands.
And in three, two, one, we're going to see an immersive hologram appear, a very realistic hologram appears in front of me, coming from the glasses I'm wearing.
Certainly, this could be something we purchase or learn, and I can use my hands to move them precisely.
"I think Iron Man would be proud."
We'll come back to this later.
Now, if you're like me, your mind is already blown by the possibilities we have with this kind of technology, let's look at some of them.
My mother is an architect, naturally the first thing I imagined was drawing a 3D building instead of using these 2D plans.
Here, she touches the graphics and selects an indoor setting.
Everything was was filmed with a GoPro using our own glasses.
This upcoming case is truly special for me, it refers to Professor Adam Gazzaley's brain glasses project, provided by the UCSF.
As a neuroscience student, I would always fantasize about the possibility of learning and memorizing these complex cerebral structures with a real machine, I could touch and play with these different cerebral structures.
What you see now is called augmented reality, but for me, it's part of a much bigger story - a story about how we start to extend our bodies through digital objects, rather than the other way around.
Now...
I think that in the coming years, humanity will experience a change.
We are going to start overlaying a whole layer of digital information onto the real world.
Imagine for a moment what it would mean for storytellers, painters, neurosurgeons, interior decorators, and perhaps for us all here today.
And what we need to do as a community is to try and make the effort to imagine how we can create this new reality in a way that extends human experience, rather than gamifying our reality or cluttering it with digital information.
This is what truly fascinates me.
Now, I want to tell you a little secret.
In about five years - not the smallest device - in about five years, they will all look like bands of glass on our eyes projecting holograms.
As we rarely consider a phone's purchase for its technical specifications - we buy it for the operating system - as a neuroscientist, I've always dreamed of creating "iOS of the mind", if you will.
It is extremely crucial to do things right, as we would be living with these things for at least as long as we spent time with the Windows user interface.
And I don't know about you, but living inside of Windows scares me.
To isolate from infinity the most intuitive interface, we use neuroscience to create our design principles, rather than a group of designers arguing in meetings.
The principle we all rely on is known as the 'Neural Path of Least Resistance.'
Every time, we try to connect our brain's iOS with our brain according to, for the first time, our conditions.
In other words, we are trying to create a computer without a learning curve.
We build a system that you know how to use all the time.
Here is the first of the three principles we use in this entirely new user experience.
Firstly, you are the operating system.
Classic file systems are complex and abstract, requiring your brain to go through increasingly more steps to decode them.
We oppose the Path Neural of Least Resistance.
While in augmented reality, you place your TED holographic panel there, and your holographic email on the other side of the desk, and your advanced spatial memory retrieves them.
You could put your holographic Tesla that you purchase - or any model that my legal team told me to put.
"Perfect. Your brain knows exactly how to bring it back."
The second interface principle is called "Touch to See".
What do babies do when they see something that catches their attention?
"Theytry to reachach it, to touch it."
That's exactly how a natural machine should operate.
It turns out that the visual system has a crucial help from a sense called proprioception - the sense of our body parts in space.
By directly touching our work, we will not only better control it, but we will also better understand it.
And thus, touch to see.
But that's not enough to feel things.
We are inherently social primates.
This leads us to our third principle, the holographic campfire from our first story.
Our mirror neuron system suggests that we can better connect with each other and our work if we can see our faces and hands in 3D.
So if you look at the video behind me, you can see two Meta users interacting with the same hologram, establishing visual contact, connected around this object, instead of being distracted by external devices.
Let's continue and try again with neuroscience in mind.
Once again, our favorite interface, the 'iOS of the Mind'.
I will now go further and grab this pair of glasses and leave it here beside the desk.
I am with you, at this moment, we are connected.
My spatial memory has kicked in, and I can grab it and put it back here, reminding myself that I am the operating system.
"And my proprioception is working, and I can continue and shatter these glasses into thousands of pieces and touch the sensor that is scanning my hand."
But that's not enough to see objects alone, in a second, the co-founder Ray will call in 3D - Ray?
"Hello Ray, how are you?"
I can see this person completely in front of me in 3D.
"And it is rather realistic."
Thank you.
My mirror neuron system suggests that this will replace phones soon.
"Ray, how are you?"
Ray: Very well. We're here live today.
MG: Ray, give the audience a gift from the holographic brain we saw earlier.
This will won't just replace phones, but it will change also our way of collaborating.
Thank you very much.
"Thank you, Ray."
Ray: You're welcome.
MG: Alright, here's the message I found out in a bar in 2011: the future of computers isn't't blocked inside one of these screens.
It's just there for us.
If there's one idea I can impart today, it's that the natural machine isn't not the product of the future, it's just here in 2016.
That's why at Meta, including administrative staff, managers, designers, engineers - before TED2011, we're all going to get rid of our external monitors and replace them with truly and deeply more natural machines.
Thank you very much.
"Thank you, that's very kind."
Thank you to everyone.
Chris Anderson: Alright, let me explain something, because there were few demonstrations of augmented reality last year or previously here.
"And sometimes there is a debate among technologists to know if we really see the real object on the screen."
It's a problem of field of view. In a way, the technology shows a wider view than what you should see while wearing the glasses.
"Have we seen the real life?"
MG: Absolutely real life.
And not only that, we have taken other measures to film with a GoPro using the real lenses, the different videos broadcasted.
"We want to try to simulate the experience of the world we see through the glasses, and not cut any part of it."
Chris Anderson: Thank you very much for this presentation.
MG: Thank you very much, it was a pleasure.
